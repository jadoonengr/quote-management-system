{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb0e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from statistics import mode\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318c9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Normalization Functions ---\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Removes HTML markup (e.g., <p>, <a> tags) that might be in the text.\"\"\"\n",
    "    # Pattern to match anything between < and > non-greedily\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes leading/trailing spaces and replaces multiple internal spaces, \n",
    "    tabs, or newlines with a single space.\n",
    "    \"\"\"\n",
    "    # Replace all sequences of whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip any leading or trailing space left over\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes Unicode characters by decomposing accented characters into \n",
    "    base characters and their diacritics, then removes the diacritics.\n",
    "    \n",
    "    Example: 'résumé' -> 'resume'\n",
    "             'François' -> 'Francois'\n",
    "             'â' -> 'a'\n",
    "    \"\"\"\n",
    "    # 1. Normalize the string to the 'NFKD' form (Compatibility Decomposition)\n",
    "    # This separates base characters from diacritical marks.\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # 2. Encode to ASCII, ignoring errors (this drops the diacritics)\n",
    "    # Then decode back to UTF-8\n",
    "    # This specifically removes non-spacing marks that were separated by NFKD.\n",
    "    return normalized.encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# --- 2. Removal Functions ---\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Removes all standard English punctuation marks.\"\"\"\n",
    "    # Creates a translation table mapping every punctuation character to None (removal)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\'\",\"'\").strip()\n",
    "\n",
    "def remove_digits(text: str) -> str:\n",
    "    \"\"\"Removes all numerical digits (0-9) from the text.\"\"\"\n",
    "    # \\d+ matches one or more digits\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes common English stop words. \n",
    "    \n",
    "    NOTE: For production NLP, use a library like NLTK or spaCy for a more \n",
    "    comprehensive and efficient stop word list.\n",
    "    \"\"\"\n",
    "    # Simple, small list of very common stop words\n",
    "    stop_words = set([\n",
    "        \"a\", \"an\", \"the\", \"is\", \"are\", \"and\", \"or\", \"to\", \"of\", \"in\", \n",
    "        \"for\", \"on\", \"with\", \"it\", \"that\", \"this\", \"but\", \"by\"\n",
    "    ])\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Clean OCR Function\n",
    "def clean_ocr_text(text: str) -> str:\n",
    "    text = normalize_accents(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68736130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "\n",
    "def get_char_vector(word: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Creates a character frequency vector (Bag-of-Characters) for a given word.\n",
    "    Example: 'apple' -> {'a': 1, 'p': 2, 'l': 1, 'e': 1}\n",
    "    \"\"\"\n",
    "    return Counter(word.lower())\n",
    "\n",
    "def cosine_similarity(word1: str, word2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two words based on their \n",
    "    Bag-of-Characters (character frequency vectors).\n",
    "    \n",
    "    The cosine similarity measures the cosine of the angle between two \n",
    "    non-zero vectors, which is a common metric for similarity.\n",
    "    \n",
    "    Args:\n",
    "        word1: The first word (string).\n",
    "        word2: The second word (string).\n",
    "        \n",
    "    Returns:\n",
    "        A float representing the cosine similarity, ranging from 0.0 (no similarity) \n",
    "        to 1.0 (identical words/vectors).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle edge cases (empty strings)\n",
    "    if not word1 or not word2:\n",
    "        return 0.0\n",
    "    \n",
    "    # 2. Create frequency vectors (Bag-of-Characters)\n",
    "    vec1 = get_char_vector(word1)\n",
    "    vec2 = get_char_vector(word2)\n",
    "    \n",
    "    # 3. Determine the set of all unique characters across both words\n",
    "    all_chars = set(vec1.keys()) | set(vec2.keys())\n",
    "    \n",
    "    # 4. Create the full numerical vectors (lists) ordered by all_chars\n",
    "    #    This ensures both vectors are aligned in the same dimension space.\n",
    "    \n",
    "    # Dot product components\n",
    "    numerator = 0\n",
    "    \n",
    "    # Squared sum components (for the denominator magnitude calculation)\n",
    "    sum_sq1 = 0\n",
    "    sum_sq2 = 0\n",
    "    \n",
    "    for char in all_chars:\n",
    "        count1 = vec1.get(char, 0)\n",
    "        count2 = vec2.get(char, 0)\n",
    "        \n",
    "        # Calculate Dot Product (A * B)\n",
    "        numerator += count1 * count2\n",
    "        \n",
    "        # Calculate Magnitude Squared (|A|^2 and |B|^2)\n",
    "        sum_sq1 += count1 ** 2\n",
    "        sum_sq2 += count2 ** 2\n",
    "        \n",
    "    # 5. Calculate Magnitudes (|A| and |B|)\n",
    "    # We use np.sqrt, but math.sqrt works too if you prefer not to use numpy\n",
    "    # If the function must strictly avoid numpy, use math.sqrt:\n",
    "    magnitude1 = math.sqrt(sum_sq1)\n",
    "    magnitude2 = math.sqrt(sum_sq2)\n",
    "    \n",
    "    # 6. Calculate Cosine Similarity\n",
    "    \n",
    "    # Check for zero magnitude (shouldn't happen if edge cases handled, but safe practice)\n",
    "    denominator = magnitude1 * magnitude2\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# PERSONAL KEYWORDS\n",
    "############################################\n",
    "personal_words = [\"dwelling\", \"homeowner\", \n",
    "                  \"spouse\", \"dependent\",\n",
    "                  \"renter\", \"lease\",\n",
    "                  \"pleasure\", \"commuting\"]\n",
    "\n",
    "personal_bigrams = [\"personal auto\",\n",
    "                    \"personal car\", \"private car\", \n",
    "                    \"primary residence\",\n",
    "                    \"principal residence\", \"personal use\", \n",
    "                    \"occupancy type\", \"uninsured motorist\",\n",
    "                    \"comprehensive coverage\",\n",
    "                    \"replacement cost\", \"single family\", \n",
    "                    \"private passenger\"]\n",
    "\n",
    "\n",
    "\n",
    "personal_trigrams = [\"homeowner's policy number\", \n",
    "                     \"medical payments to\", \n",
    "                     \"any household members\", \n",
    "                     \"additional interest mortgagee\",\n",
    "                     \"social domestic pleasure\", \n",
    "                     \"auto liability bodily\", \n",
    "                     \"homeowners insurance policy\", \n",
    "                     \"personal articles floater\"]\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "# COMMERCIAL KEYWORDS\n",
    "############################################\n",
    "commercial_words = [\"commercial\",\n",
    "              \"fiduciary\", \"naics\", \n",
    "              \"fein\", \"employee/empl\",\n",
    "              \"workers\",\n",
    "              \"client\", \"fleet\", \n",
    "              \"inventory\", \"proprietor\", \n",
    "              \"exposure\", \"payroll\", \n",
    "              \"entity\", \"vendor\",\n",
    "              \"revenues\", \"wholesale\", \n",
    "              \"contractors\", \"llc\"]\n",
    "\n",
    "\n",
    "commercial_bigrams = [\"commercial general\", \"business auto\", \n",
    "                \"gl code\", \"sic code\", \"business owner's\",\n",
    "                \"general liability\", \"commercial auto\", \n",
    "                \"workers' comp\", \"business interruption\", \n",
    "                \"additional insured\", \"aggregate limit\", \n",
    "                \"acord 125\", \"motor carrier\", \n",
    "                \"part time\", \"annual revenues\"]\n",
    "\n",
    "\n",
    "\n",
    "commercial_trigrams = [\"business personal property\",\n",
    "                 \"masonry veneer construction\",\n",
    "                 \"certificate of insurance\",\n",
    "                 \"commercial general liability\", \n",
    "                 \"fein or soc\", \n",
    "                 \"description of operations\" \n",
    "                 \"boiler & machinery\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# BIGRAMS\n",
    "personal_auto_bigrams = [\"personal auto\", \"private auto\", \n",
    "                        \"personal car\", \"private car\", \n",
    "                        \"Primary Driver\", \"Household Members\",\n",
    "                        \"Commute Distance\", \"Teen Driver\",\n",
    "                        \"Personal Use\",\"Multi-Car\",\n",
    "                        \"Pleasure Use\",\"Family Vehicle\",\n",
    "                        \"Occasional Driver\",\"Good Student\",\n",
    "                        \"Student Discount\",\"Uninsured Motorist\",\n",
    "                        \"Home Address\"]\n",
    "\n",
    "commercial_auto_bigrams = [\"commercial auto\", \"commercial car\",\n",
    "                          \"Federal Tax\",\"Motor Carrier\",\n",
    "                          \"Cargo Coverage\",\"Company Name\",\n",
    "                          \"DOT Number\",\"Gross Vehicle\",\n",
    "                          \"For Hire\",\"Business Operations\",\n",
    "                          \"Employee Driver\",\"Fleet Size\",\n",
    "                          \"Hazardous Materials\",\"Terminal Address\"]\n",
    "\n",
    "\n",
    "\n",
    "personal_property_bigrams = [\"personal property\", \"private property\",\n",
    "                             \"Dwelling Location\", \"Replacement Cost\", \"Roof Age\", \"Fire Protection Class\", \"Swimming Pool\", \"Maisonry\"\n",
    "                             ]\n",
    "commercial_property_bigrams = [\"commercial property\"]\n",
    "\n",
    "\n",
    "# TRIGRAMS\n",
    "personal_auto_trigrams = [\"Number of Drivers\",\"High School Diploma\",\n",
    "\"Safe Driver Discount\",\"Annual Mileage Driven\",\n",
    "\"Driving Record History\",\"Coverage for Rental\",\n",
    "\"Primary Garaging Location\",\"Anti-Theft Device\",\n",
    "\"Resident of Household\"]\n",
    "\n",
    "\n",
    "commercial_auto_trigrams = [\"commercial fleet insurance\",\"Gross Vehicle Weight\",\n",
    "\"Interstate Commerce Commission\",\"Business Use Only\",\n",
    "\"Operating Radius Limit\",\"Combined Single Limit\",\n",
    "\"Certificate of Insurance\",\"Number of Employees\",\n",
    "\"Non-Owned Hired Auto\",\"Unified Carrier Registration\",\n",
    "\"Products Completed Operations\"]\n",
    "\n",
    "personal_property_trigrams = [\"personal property\", \"private property\"]\n",
    "commercial_property_trigrams = [\"commercial property\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfac9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Loads a PDF file from a local repository into a bytes object.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        The content of the PDF file as bytes.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"Error: The file was not found at {file_path}\")\n",
    "\n",
    "    # print(f\"Loading file: {path.name} ({os.path.getsize(path)} bytes)\")\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    return pdf_bytes\n",
    "\n",
    "\n",
    "def semantic_search(target_list:list, text_list:list):\n",
    "    temp = []\n",
    "    for target_label in target_list:\n",
    "        for txt in text_list:\n",
    "            threshold = cosine_similarity(target_label, txt)\n",
    "            if threshold > 0.97: \n",
    "                temp.append(txt)\n",
    "                print(f\"Match found: {target_label} <-> {txt} (Similarity: {threshold:.4f})\")\n",
    "\n",
    "        if len(temp) > 0: return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4adcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_classify(pdf_bytes: bytes):\n",
    "    \"\"\"\n",
    "    Processes the PDF bytes using pypdfium2 to extract text from the first page.\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes: The PDF content as bytes.\n",
    "    \"\"\"\n",
    "    # print(\"Processing PDF content with pypdfium2...\")\n",
    "\n",
    "    # 1. Load the PDF document directly from the bytes object\n",
    "    # pypdfium2's PdfDocument.open() handles byte streams naturally.\n",
    "    try:\n",
    "        pdf_document = pypdfium2.PdfDocument(pdf_bytes)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open PDF from bytes. Ensure the file is a valid PDF. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    num_pages = len(pdf_document)\n",
    "    # print(f\"Document successfully loaded. Total pages: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        print(\"Document is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2. Access the first page\n",
    "    page_index = 0\n",
    "    page = pdf_document.get_page(page_index)\n",
    "\n",
    "    form_type = set()\n",
    "    claim_type = set()\n",
    "    claim_cat = set()\n",
    "\n",
    "    # 3. Use the PDF Text Page object for Logo detection\n",
    "    for i in [70, 80, 90, 100, 150, 200]:\n",
    "        bitmap = page.render(scale=300/i)\n",
    "        image = bitmap.to_pil()\n",
    "\n",
    "        # Extract Text using Tesseract\n",
    "        text1 = pytesseract.image_to_string(image)\n",
    "\n",
    "        # Clean Text\n",
    "        text2 = clean_ocr_text(text1.lower())\n",
    "        clean_text = text2.split()\n",
    "\n",
    "        # Create bi/tri-grams\n",
    "        two_word_phrases = [' '.join(pair) for pair in ngrams(clean_text, 2)]\n",
    "        three_word_phrases = [' '.join(trio) for trio in ngrams(clean_text, 3)]\n",
    "\n",
    "        # Classify personal vs commercial\n",
    "        if  (\"personal\" not in claim_cat) and \\\n",
    "            (semantic_search(personal_words, clean_text) or \\\n",
    "             semantic_search(personal_bigrams, two_word_phrases) or\\\n",
    "             semantic_search(personal_trigrams, three_word_phrases)):\n",
    "            claim_cat.add(\"personal\")\n",
    "\n",
    "        if  (\"commercial\" not in claim_cat) and \\\n",
    "            (semantic_search(commercial_words, clean_text) or \\\n",
    "             semantic_search(commercial_bigrams, two_word_phrases) or\\\n",
    "             semantic_search(commercial_trigrams, three_word_phrases)):\n",
    "            claim_cat.add(\"commercial\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if (\"personal auto\" not in claim_type) and (semantic_search(personal_auto_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"personal auto\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(commercial_auto_bigrams, two_word_phrases) or semantic_search(commercial_auto_trigrams, three_word_phrases)):\n",
    "        #     claim_type.add(\"commercial auto\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(personal_property_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"personal property\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(commercial_property_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"commercial property\")\n",
    "\n",
    "    return claim_cat\n",
    "\n",
    "\n",
    "        # text_page = page.get_textpage()\n",
    "        \n",
    "        # # Extract all text from the page\n",
    "        # text2 = text_page.get_text_range()\n",
    "\n",
    "        # # 4. Print the extracted text\n",
    "        # print(\"-\" * 50)\n",
    "        # print(f\"Extracted Text from Page {page_index + 1}:\")\n",
    "        # print(text2.strip()[:500] + ('...' if len(text2) > 500 else '')) # Print first 500 chars\n",
    "        # print(\"-\" * 50)\n",
    "    # print(f\"Claim Type is: {claim_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af2a1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recursive search in: D:\\Workspace\\projects\\quote-management-system\\sample_forms\n",
      "Found 32 PDF file(s).\n",
      "\n",
      "--- File 1/32: ..\\sample_forms\\auto\\Aviva-private-car-insurance-proposal-form.pdf ---\n",
      "Match found: private car <-> private car (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 2/32: ..\\sample_forms\\auto\\CSIO-Commercial-Fleet-application-form.pdf ---\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 3/32: ..\\sample_forms\\auto\\Maritime-Motor-Insurance-Quotation-Form.pdf ---\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 4/32: ..\\sample_forms\\auto_commercial\\ACORD-Business-Auto-Section-127.pdf ---\n",
      "Match found: business auto <-> business auto (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 5/32: ..\\sample_forms\\auto_commercial\\Acord125 Commercial App.pdf ---\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 6/32: ..\\sample_forms\\auto_commercial\\Commonwealth-commercial-quote_info_sheet.pdf ---\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 7/32: ..\\sample_forms\\auto_commercial\\Truckers-Quick-Quote-Sheet.pdf ---\n",
      "Match found: payroll <-> payroll (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 8/32: ..\\sample_forms\\auto_personal\\Acord-71.pdf ---\n",
      "Match found: personal auto <-> personal auto (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 9/32: ..\\sample_forms\\auto_personal\\Acord-83-Personal-Umbrella.pdf ---\n",
      "Match found: personal auto <-> auto personal (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 10/32: ..\\sample_forms\\auto_personal\\Acord-90-CA.pdf ---\n",
      "Match found: dependent <-> dependents (Similarity: 0.9747)\n",
      "{'personal'}\n",
      "\n",
      "--- File 11/32: ..\\sample_forms\\auto_personal\\Acord-90-KS.pdf ---\n",
      "Match found: personal auto <-> personal auto (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 12/32: ..\\sample_forms\\auto_personal\\Acord-90-MI.pdf ---\n",
      "Match found: dependent <-> dependents (Similarity: 0.9747)\n",
      "{'personal'}\n",
      "\n",
      "--- File 13/32: ..\\sample_forms\\auto_personal\\acord-90-NY-fillable.pdf ---\n",
      "Match found: personal auto <-> personal auto (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 14/32: ..\\sample_forms\\auto_personal\\Acord-90-NY.pdf ---\n",
      "Match found: dependent <-> dependents (Similarity: 0.9747)\n",
      "{'personal'}\n",
      "\n",
      "--- File 15/32: ..\\sample_forms\\auto_personal\\Auto Insurance Quote Form.pdf ---\n",
      "Match found: year make model <-> year make model (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 16/32: ..\\sample_forms\\auto_personal\\Auto Insurance Quote Request.pdf ---\n",
      "Match found: uninsured motorist <-> uninsured motorist (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 17/32: ..\\sample_forms\\auto_personal\\Auto-Quote-Example.pdf ---\n",
      "Match found: personal auto <-> personal auto (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 18/32: ..\\sample_forms\\auto_personal\\Auto-quote-TX-Harmon.pdf ---\n",
      "Match found: year make model <-> yr make model (Similarity: 0.9713)\n",
      "{'personal'}\n",
      "\n",
      "--- File 19/32: ..\\sample_forms\\auto_personal\\Kleve-Auto-Quote-OH.pdf ---\n",
      "Match found: year make model <-> year make model (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 20/32: ..\\sample_forms\\auto_personal\\National-General-Auto-Quote.pdf ---\n",
      "Match found: personal auto <-> personal auto (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 21/32: ..\\sample_forms\\auto_personal\\State-Farm-Auto-Quotes.pdf ---\n",
      "Match found: uninsured motorist <-> underinsured motorist (Similarity: 0.9705)\n",
      "{'personal'}\n",
      "\n",
      "--- File 22/32: ..\\sample_forms\\property_commercial\\Acord-140-Property.pdf ---\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 23/32: ..\\sample_forms\\property_commercial\\Acord-140.pdf ---\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "Match found: exposure <-> exposure (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 24/32: ..\\sample_forms\\property_commercial\\Acord125CommInsApp.pdf ---\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "Match found: commercial <-> commercial (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 25/32: ..\\sample_forms\\property_commercial\\Business Insurance Quote Request Form.pdf ---\n",
      "Match found: payroll <-> payroll (Similarity: 1.0000)\n",
      "{'commercial'}\n",
      "\n",
      "--- File 26/32: ..\\sample_forms\\property_personal\\Acord-80-Personal-Property-filled.pdf ---\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: loss of use <-> loss of use (Similarity: 1.0000)\n",
      "{'personal', 'commercial'}\n",
      "\n",
      "--- File 27/32: ..\\sample_forms\\property_personal\\Acord-80-Personal-Property.pdf ---\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: loss of use <-> loss of use (Similarity: 1.0000)\n",
      "{'personal', 'commercial'}\n",
      "\n",
      "--- File 28/32: ..\\sample_forms\\property_personal\\Buffalo-homeowners-quote.pdf ---\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 29/32: ..\\sample_forms\\property_personal\\Home-Quote-Comparison-Example.pdf ---\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "Match found: loss of use <-> loss of use (Similarity: 1.0000)\n",
      "{'personal', 'commercial'}\n",
      "\n",
      "--- File 30/32: ..\\sample_forms\\property_personal\\Homeowners Insurance Quote Form.pdf ---\n",
      "Match found: spouse <-> spouse (Similarity: 1.0000)\n",
      "Match found: spouse <-> spouse (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 31/32: ..\\sample_forms\\property_personal\\Renters Insurance Quote.pdf ---\n",
      "Match found: dwelling <-> dwelling (Similarity: 1.0000)\n",
      "{'personal'}\n",
      "\n",
      "--- File 32/32: ..\\sample_forms\\property_personal\\Student Accident Insurance Quote Request Form.pdf ---\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "root_path = Path(\"../sample_forms/\")\n",
    "\n",
    "if not root_path.is_dir():\n",
    "    print(f\"Error: Root directory not found\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Starting recursive search in: {root_path.resolve()}\")\n",
    "\n",
    "# Use glob with '**/*.pdf' for recursive search for all files ending in .pdf\n",
    "pdf_files = list(root_path.glob('**/*.pdf'))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"No PDF files found.\")\n",
    "    exit(0)\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF file(s).\")\n",
    "pdf_paths = list(map(str, pdf_files))\n",
    "file_classify = pd.DataFrame(pdf_paths, columns=[\"File_Path\"])\n",
    "file_classify[\"Claim_Type\"] = None\n",
    "file_classify[\"Form_Type\"] = None\n",
    "\n",
    "for i in range(len(pdf_paths)):\n",
    "    print(f\"\\n--- File {i+1}/{len(pdf_paths)}: {pdf_paths[i]} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load the entire file content into a bytes object using pathlib's method\n",
    "        pdf_bytes = load_pdf(pdf_paths[i])\n",
    "        \n",
    "        # Process the loaded bytes\n",
    "        # print(process_pdf(pdf_bytes))\n",
    "        val = quote_classify(pdf_bytes)\n",
    "        print(val)\n",
    "        # file_classify.at[i,\"Claim_Category\"] = val\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"    [SKIP] Permission denied when accessing {pdf_paths[i]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [ERROR] An unexpected error occurred while reading {pdf_paths[i]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b838317",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f349d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pdf_paths[14]\n",
    "print(a)\n",
    "b = load_pdf(a)\n",
    "c = process_pdf(b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
