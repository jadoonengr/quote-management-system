{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb0e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from statistics import mode\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318c9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Normalization Functions ---\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Removes HTML markup (e.g., <p>, <a> tags) that might be in the text.\"\"\"\n",
    "    # Pattern to match anything between < and > non-greedily\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes leading/trailing spaces and replaces multiple internal spaces, \n",
    "    tabs, or newlines with a single space.\n",
    "    \"\"\"\n",
    "    # Replace all sequences of whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip any leading or trailing space left over\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes Unicode characters by decomposing accented characters into \n",
    "    base characters and their diacritics, then removes the diacritics.\n",
    "    \n",
    "    Example: 'résumé' -> 'resume'\n",
    "             'François' -> 'Francois'\n",
    "             'â' -> 'a'\n",
    "    \"\"\"\n",
    "    # 1. Normalize the string to the 'NFKD' form (Compatibility Decomposition)\n",
    "    # This separates base characters from diacritical marks.\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # 2. Encode to ASCII, ignoring errors (this drops the diacritics)\n",
    "    # Then decode back to UTF-8\n",
    "    # This specifically removes non-spacing marks that were separated by NFKD.\n",
    "    return normalized.encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# --- 2. Removal Functions ---\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Removes all standard English punctuation marks.\"\"\"\n",
    "    # Creates a translation table mapping every punctuation character to None (removal)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\'\",\"'\").strip()\n",
    "\n",
    "def remove_digits(text: str) -> str:\n",
    "    \"\"\"Removes all numerical digits (0-9) from the text.\"\"\"\n",
    "    # \\d+ matches one or more digits\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes common English stop words. \n",
    "    \n",
    "    NOTE: For production NLP, use a library like NLTK or spaCy for a more \n",
    "    comprehensive and efficient stop word list.\n",
    "    \"\"\"\n",
    "    # Simple, small list of very common stop words\n",
    "    stop_words = set([\n",
    "        \"a\", \"an\", \"the\", \"is\", \"are\", \"and\", \"or\", \"to\", \"of\", \"in\", \n",
    "        \"for\", \"on\", \"with\", \"it\", \"that\", \"this\", \"but\", \"by\"\n",
    "    ])\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Clean OCR Function\n",
    "def clean_ocr_text(text: str) -> str:\n",
    "    text = normalize_accents(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68736130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "\n",
    "def get_char_vector(word: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Creates a character frequency vector (Bag-of-Characters) for a given word.\n",
    "    Example: 'apple' -> {'a': 1, 'p': 2, 'l': 1, 'e': 1}\n",
    "    \"\"\"\n",
    "    return Counter(word.lower())\n",
    "\n",
    "def cosine_similarity(word1: str, word2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two words based on their \n",
    "    Bag-of-Characters (character frequency vectors).\n",
    "    \n",
    "    The cosine similarity measures the cosine of the angle between two \n",
    "    non-zero vectors, which is a common metric for similarity.\n",
    "    \n",
    "    Args:\n",
    "        word1: The first word (string).\n",
    "        word2: The second word (string).\n",
    "        \n",
    "    Returns:\n",
    "        A float representing the cosine similarity, ranging from 0.0 (no similarity) \n",
    "        to 1.0 (identical words/vectors).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle edge cases (empty strings)\n",
    "    if not word1 or not word2:\n",
    "        return 0.0\n",
    "    \n",
    "    # 2. Create frequency vectors (Bag-of-Characters)\n",
    "    vec1 = get_char_vector(word1)\n",
    "    vec2 = get_char_vector(word2)\n",
    "    \n",
    "    # 3. Determine the set of all unique characters across both words\n",
    "    all_chars = set(vec1.keys()) | set(vec2.keys())\n",
    "    \n",
    "    # 4. Create the full numerical vectors (lists) ordered by all_chars\n",
    "    #    This ensures both vectors are aligned in the same dimension space.\n",
    "    \n",
    "    # Dot product components\n",
    "    numerator = 0\n",
    "    \n",
    "    # Squared sum components (for the denominator magnitude calculation)\n",
    "    sum_sq1 = 0\n",
    "    sum_sq2 = 0\n",
    "    \n",
    "    for char in all_chars:\n",
    "        count1 = vec1.get(char, 0)\n",
    "        count2 = vec2.get(char, 0)\n",
    "        \n",
    "        # Calculate Dot Product (A * B)\n",
    "        numerator += count1 * count2\n",
    "        \n",
    "        # Calculate Magnitude Squared (|A|^2 and |B|^2)\n",
    "        sum_sq1 += count1 ** 2\n",
    "        sum_sq2 += count2 ** 2\n",
    "        \n",
    "    # 5. Calculate Magnitudes (|A| and |B|)\n",
    "    # We use np.sqrt, but math.sqrt works too if you prefer not to use numpy\n",
    "    # If the function must strictly avoid numpy, use math.sqrt:\n",
    "    magnitude1 = math.sqrt(sum_sq1)\n",
    "    magnitude2 = math.sqrt(sum_sq2)\n",
    "    \n",
    "    # 6. Calculate Cosine Similarity\n",
    "    \n",
    "    # Check for zero magnitude (shouldn't happen if edge cases handled, but safe practice)\n",
    "    denominator = magnitude1 * magnitude2\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfac9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_as_bytes(file_path: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Loads a PDF file from a local repository into a bytes object.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        The content of the PDF file as bytes.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"Error: The file was not found at {file_path}\")\n",
    "\n",
    "    print(f\"Loading file: {path.name} ({os.path.getsize(path)} bytes)\")\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    return pdf_bytes\n",
    "\n",
    "\n",
    "def check_label(target_label:str, text_list:list):\n",
    "    target_label = target_label.lower()\n",
    "    temp = []\n",
    "    for txt in text_list:\n",
    "        threshold = cosine_similarity(target_label, txt)\n",
    "        if threshold > 0.95: temp.append(txt)\n",
    "\n",
    "    if len(temp) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4405c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('auto', 'personal'), ('property', 'commercial')),\n",
       " (('property', 'commercial'), ('auto', 'personal'))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "a = [\"auto\", \"property\"]\n",
    "b = [\"personal\", \"commercial\"]\n",
    "\n",
    "#It generate all possible combinations of elements from both lists\n",
    "# Convert the result into a list of tuples\n",
    "list(itertools.permutations(zip(a, b), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "word_a = \"apple\"\n",
    "word_b = \"apply\"\n",
    "word_c = \"orange\"\n",
    "word_d = \"aPpLe\"\n",
    "word_e = \"\"\n",
    "print(check_label(\"apple1\", [word_a]))\n",
    "\n",
    "auto_personal_lst = [\"auto personal\", \"auto private\", \"private \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea4adcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_from_bytes(pdf_bytes: bytes):\n",
    "    \"\"\"\n",
    "    Processes the PDF bytes using pypdfium2 to extract text from the first page.\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes: The PDF content as bytes.\n",
    "    \"\"\"\n",
    "    print(\"Processing PDF content with pypdfium2...\")\n",
    "\n",
    "    # 1. Load the PDF document directly from the bytes object\n",
    "    # pypdfium2's PdfDocument.open() handles byte streams naturally.\n",
    "    try:\n",
    "        pdf_document = pypdfium2.PdfDocument(pdf_bytes)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open PDF from bytes. Ensure the file is a valid PDF. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    num_pages = len(pdf_document)\n",
    "    print(f\"Document successfully loaded. Total pages: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        print(\"Document is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2. Access the first page\n",
    "    page_index = 0\n",
    "    page = pdf_document.get_page(page_index)\n",
    "\n",
    "    form_type = []\n",
    "    claim_type = []\n",
    "\n",
    "    # 3. Use the PDF Text Page object for Logo detection\n",
    "    for i in [70, 80, 90, 100, 150, 200]:\n",
    "        bitmap = page.render(scale=300/i)\n",
    "        image = bitmap.to_pil()\n",
    "\n",
    "        # Extract Text using Tesseract\n",
    "        text = pytesseract.image_to_string(image)\n",
    "\n",
    "        # Clean Text\n",
    "        text = clean_ocr_text(text.lower())\n",
    "        clean_text = text.split()\n",
    "\n",
    "        # Create bi/tri-grams\n",
    "        two_word_phrases = [' '.join(pair) for pair in ngrams(clean_text, 2)]\n",
    "        three_word_phrases = [' '.join(trio) for trio in ngrams(clean_text, 3)]\n",
    "\n",
    "        if check_label(\"Personal Auto\", two_word_phrases):\n",
    "            claim_type.append(\"auto personal\")\n",
    "        elif check_label(\"Commercial Auto\", two_word_phrases):\n",
    "            claim_type.append(\"auto commercial\")\n",
    "        elif check_label(\"Personal Property\", two_word_phrases):\n",
    "            claim_type.append(\"property personal\")\n",
    "        elif check_label(\"Commercial Property\", two_word_phrases):\n",
    "            claim_type.append(\"property commercial\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # text_page = page.get_textpage()\n",
    "        \n",
    "        # # Extract all text from the page\n",
    "        # text2 = text_page.get_text_range()\n",
    "\n",
    "        # # 4. Print the extracted text\n",
    "        # print(\"-\" * 50)\n",
    "        # print(f\"Extracted Text from Page {page_index + 1}:\")\n",
    "        # print(text2.strip()[:500] + ('...' if len(text2) > 500 else '')) # Print first 500 chars\n",
    "        # print(\"-\" * 50)\n",
    "    print(f\"Claim Type is: {claim_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af2a1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recursive search in: /home/aamir79/projects/quote-management-system/sample_forms\n",
      "Found 32 PDF file(s).\n",
      "\n",
      "--- File 1/32: ../sample_forms/auto/Aviva-private-car-insurance-proposal-form.pdf ---\n",
      "Loading file: Aviva-private-car-insurance-proposal-form.pdf (222930 bytes)\n",
      "Processing PDF content with pypdfium2...\n",
      "Document successfully loaded. Total pages: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     pdf_bytes \u001b[38;5;241m=\u001b[39m load_pdf_as_bytes(pdf_path)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Process the loaded bytes\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mprocess_pdf_from_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    [SKIP] Permission denied when accessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36mprocess_pdf_from_bytes\u001b[0;34m(pdf_bytes)\u001b[0m\n\u001b[1;32m     45\u001b[0m two_word_phrases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pair) \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m ngrams(clean_text, \u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m     46\u001b[0m three_word_phrases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(trio) \u001b[38;5;28;01mfor\u001b[39;00m trio \u001b[38;5;129;01min\u001b[39;00m ngrams(clean_text, \u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcheck_label\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPersonal Auto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwo_word_phrases\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     49\u001b[0m     claim_type\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto personal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m check_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommercial Auto\u001b[39m\u001b[38;5;124m\"\u001b[39m, two_word_phrases):\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mcheck_label\u001b[0;34m(target_label, text_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m temp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m text_list:\n\u001b[0;32m---> 30\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m(target_label, txt)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.95\u001b[39m: temp\u001b[38;5;241m.\u001b[39mappend(txt)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mcheck_label\u001b[0;34m(target_label, text_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m temp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m text_list:\n\u001b[0;32m---> 30\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m(target_label, txt)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.95\u001b[39m: temp\u001b[38;5;241m.\u001b[39mappend(txt)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/quote-management-system/env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2188\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2190\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/quote-management-system/env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2255\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2257\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2258\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "root_path = Path(\"../sample_forms/\")\n",
    "\n",
    "if not root_path.is_dir():\n",
    "    print(f\"Error: Root directory not found\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Starting recursive search in: {root_path.resolve()}\")\n",
    "\n",
    "# Use glob with '**/*.pdf' for recursive search for all files ending in .pdf\n",
    "pdf_files = list(root_path.glob('**/*.pdf'))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"No PDF files found.\")\n",
    "    exit(0)\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF file(s).\")\n",
    "\n",
    "for i, pdf_path in enumerate(pdf_files):\n",
    "    print(f\"\\n--- File {i+1}/{len(pdf_files)}: {pdf_path} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load the entire file content into a bytes object using pathlib's method\n",
    "        pdf_bytes = load_pdf_as_bytes(pdf_path)\n",
    "        \n",
    "        # Process the loaded bytes\n",
    "        process_pdf_from_bytes(pdf_bytes)\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"    [SKIP] Permission denied when accessing {pdf_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [ERROR] An unexpected error occurred while reading {pdf_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b838317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3624a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: You must set the path to the Tesseract executable \n",
    "# if it is not automatically found by pytesseract.\n",
    "# \n",
    "# Example for Windows:\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def ocr_scanned_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs OCR on a scanned PDF file and returns the extracted text.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The file path to the scanned PDF.\n",
    "\n",
    "    Returns:\n",
    "        A string containing all text extracted from the PDF, \n",
    "        or an error message if the file cannot be processed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return f\"Error: PDF file not found at {pdf_path}\"\n",
    "\n",
    "    try:\n",
    "        # Load the PDF file using pypdfium2\n",
    "        pdf_document = pdfium.PdfDocument(pdf_path)\n",
    "        num_pages = len(pdf_document)\n",
    "        full_text = []\n",
    "\n",
    "        print(f\"Starting OCR on {num_pages} pages...\")\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = pdf_document.get_page(i)\n",
    "            \n",
    "            # Render the page to a bitmap (image)\n",
    "            # Scale factor 2 provides good resolution for OCR\n",
    "            bitmap = page.render(scale=2)\n",
    "            \n",
    "            # Convert the bitmap to a PIL Image object\n",
    "            image = bitmap.to_pil()\n",
    "            \n",
    "            # --- OCR Processing ---\n",
    "            # Use pytesseract to extract text from the image\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            \n",
    "            print(f\"--- Page {i+1} OCR Completed ---\")\n",
    "            full_text.append(text)\n",
    "\n",
    "        return \"\\n\".join(full_text)\n",
    "\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        return \"Error: Tesseract is not installed or not in your PATH. Please install it or set 'pytesseract.pytesseract.tesseract_cmd'.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during processing: {e}\"\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'path/to/your/scanned_document.pdf' with your actual file path\n",
    "    pdf_file = 'path/to/your/scanned_document.pdf' \n",
    "    \n",
    "    # --- IMPORTANT: Ensure you have a scanned PDF file here for testing ---\n",
    "    # For demonstration, we'll use a placeholder path:\n",
    "    # If you want to test this, create a simple PDF with a picture of text.\n",
    "    \n",
    "    # Example using a dummy file path (update this):\n",
    "    dummy_pdf_file = \"sample_scanned_document.pdf\" \n",
    "    \n",
    "    # Assuming 'sample_scanned_document.pdf' exists and is a scanned image PDF\n",
    "    if os.path.exists(dummy_pdf_file):\n",
    "        extracted_text = ocr_scanned_pdf(dummy_pdf_file)\n",
    "        \n",
    "        print(\"\\n====================================\")\n",
    "        print(\"         EXTRACTED TEXT\")\n",
    "        print(\"====================================\\n\")\n",
    "        print(extracted_text)\n",
    "    else:\n",
    "        print(f\"Please replace '{dummy_pdf_file}' with the path to an existing scanned PDF to run the example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
