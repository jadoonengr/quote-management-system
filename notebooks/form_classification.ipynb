{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb0e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamir79/projects/quote-management-system/env/lib/python3.10/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from statistics import mode\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318c9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Normalization Functions ---\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Removes HTML markup (e.g., <p>, <a> tags) that might be in the text.\"\"\"\n",
    "    # Pattern to match anything between < and > non-greedily\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes leading/trailing spaces and replaces multiple internal spaces, \n",
    "    tabs, or newlines with a single space.\n",
    "    \"\"\"\n",
    "    # Replace all sequences of whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip any leading or trailing space left over\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes Unicode characters by decomposing accented characters into \n",
    "    base characters and their diacritics, then removes the diacritics.\n",
    "    \n",
    "    Example: 'résumé' -> 'resume'\n",
    "             'François' -> 'Francois'\n",
    "             'â' -> 'a'\n",
    "    \"\"\"\n",
    "    # 1. Normalize the string to the 'NFKD' form (Compatibility Decomposition)\n",
    "    # This separates base characters from diacritical marks.\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # 2. Encode to ASCII, ignoring errors (this drops the diacritics)\n",
    "    # Then decode back to UTF-8\n",
    "    # This specifically removes non-spacing marks that were separated by NFKD.\n",
    "    return normalized.encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# --- 2. Removal Functions ---\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Removes all standard English punctuation marks.\"\"\"\n",
    "    # Creates a translation table mapping every punctuation character to None (removal)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\'\",\"'\").strip()\n",
    "\n",
    "def remove_digits(text: str) -> str:\n",
    "    \"\"\"Removes all numerical digits (0-9) from the text.\"\"\"\n",
    "    # \\d+ matches one or more digits\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes common English stop words. \n",
    "    \n",
    "    NOTE: For production NLP, use a library like NLTK or spaCy for a more \n",
    "    comprehensive and efficient stop word list.\n",
    "    \"\"\"\n",
    "    # Simple, small list of very common stop words\n",
    "    stop_words = set([\n",
    "        \"a\", \"an\", \"the\", \"is\", \"are\", \"and\", \"or\", \"to\", \"of\", \"in\", \n",
    "        \"for\", \"on\", \"with\", \"it\", \"that\", \"this\", \"but\", \"by\"\n",
    "    ])\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Clean OCR Function\n",
    "def clean_ocr_text(text: str) -> str:\n",
    "    text = normalize_accents(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68736130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "\n",
    "def get_char_vector(word: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Creates a character frequency vector (Bag-of-Characters) for a given word.\n",
    "    Example: 'apple' -> {'a': 1, 'p': 2, 'l': 1, 'e': 1}\n",
    "    \"\"\"\n",
    "    return Counter(word.lower())\n",
    "\n",
    "def cosine_similarity(word1: str, word2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two words based on their \n",
    "    Bag-of-Characters (character frequency vectors).\n",
    "    \n",
    "    The cosine similarity measures the cosine of the angle between two \n",
    "    non-zero vectors, which is a common metric for similarity.\n",
    "    \n",
    "    Args:\n",
    "        word1: The first word (string).\n",
    "        word2: The second word (string).\n",
    "        \n",
    "    Returns:\n",
    "        A float representing the cosine similarity, ranging from 0.0 (no similarity) \n",
    "        to 1.0 (identical words/vectors).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle edge cases (empty strings)\n",
    "    if not word1 or not word2:\n",
    "        return 0.0\n",
    "    \n",
    "    # 2. Create frequency vectors (Bag-of-Characters)\n",
    "    vec1 = get_char_vector(word1)\n",
    "    vec2 = get_char_vector(word2)\n",
    "    \n",
    "    # 3. Determine the set of all unique characters across both words\n",
    "    all_chars = set(vec1.keys()) | set(vec2.keys())\n",
    "    \n",
    "    # 4. Create the full numerical vectors (lists) ordered by all_chars\n",
    "    #    This ensures both vectors are aligned in the same dimension space.\n",
    "    \n",
    "    # Dot product components\n",
    "    numerator = 0\n",
    "    \n",
    "    # Squared sum components (for the denominator magnitude calculation)\n",
    "    sum_sq1 = 0\n",
    "    sum_sq2 = 0\n",
    "    \n",
    "    for char in all_chars:\n",
    "        count1 = vec1.get(char, 0)\n",
    "        count2 = vec2.get(char, 0)\n",
    "        \n",
    "        # Calculate Dot Product (A * B)\n",
    "        numerator += count1 * count2\n",
    "        \n",
    "        # Calculate Magnitude Squared (|A|^2 and |B|^2)\n",
    "        sum_sq1 += count1 ** 2\n",
    "        sum_sq2 += count2 ** 2\n",
    "        \n",
    "    # 5. Calculate Magnitudes (|A| and |B|)\n",
    "    # We use np.sqrt, but math.sqrt works too if you prefer not to use numpy\n",
    "    # If the function must strictly avoid numpy, use math.sqrt:\n",
    "    magnitude1 = math.sqrt(sum_sq1)\n",
    "    magnitude2 = math.sqrt(sum_sq2)\n",
    "    \n",
    "    # 6. Calculate Cosine Similarity\n",
    "    \n",
    "    # Check for zero magnitude (shouldn't happen if edge cases handled, but safe practice)\n",
    "    denominator = magnitude1 * magnitude2\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ced817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# AUTO KEYWORDS\n",
    "############################################\n",
    "auto_words = [\"car\", \"vehicle\", \"automobile\",\n",
    "              \"vin\",\"mvr\",\n",
    "              \"collision\",\"comprehensive\",\n",
    "              \"commute\",\"odometer\",\n",
    "              \"garaging\",\"make\",\n",
    "              \"model\",\"uninsured\"]\n",
    "\n",
    "auto_bigrams = [\"vehicle identification\",\n",
    "                \"anti-theft device\",\n",
    "                \"uninsured motorist\",\n",
    "                \"accident forgiveness\",\n",
    "                \"driver safety\",\n",
    "                \"good student\",\n",
    "                \"driving record\",\n",
    "                \"annual mileage\",\n",
    "                \"assigned risk\",\n",
    "                \"stated amount\"]\n",
    "\n",
    "auto_trigrams = [\"vehicle identification number\",\n",
    "                 \"uninsured motorist coverage\",\n",
    "                 \"safe driver discount\",\n",
    "                 \"financial responsibility proof\",\n",
    "                 \"defensive driving course\"]\n",
    "\n",
    "############################################\n",
    "# PROPERTY KEYWORDS\n",
    "############################################\n",
    "prop_words = [\"dwelling\",\n",
    "              \"coinsurance\",\"sqft\",\n",
    "              \"masonry\",\"frame\",\n",
    "              \"occupancy\",\"hvac\",\n",
    "              \"sprinkler\",\"hydrant\",\n",
    "              \"flood\"]\n",
    "\n",
    "prop_bigrams = [\"replacement cost\",\n",
    "                \"square footage\",\n",
    "                \"fire district\",\n",
    "                \"protection class\",\n",
    "                \"roof age\",\n",
    "                \"water backup\",\n",
    "                \"plumbing updates\",\n",
    "                \"foundation type\",\n",
    "                \"vandalism coverage\",\n",
    "                \"business income\",\n",
    "                \"personal property\"]\n",
    "\n",
    "prop_trigrams = [\"distance to fire\",\n",
    "                 \"business personal property\",\n",
    "                 \"masonry veneer construction\",\n",
    "                 \"causes of loss\",\n",
    "                 \"loss of use\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# BIGRAMS\n",
    "personal_auto_bigrams = [\"personal auto\", \"private auto\", \n",
    "                        \"personal car\", \"private car\", \n",
    "                        \"Primary Driver\", \"Household Members\",\n",
    "                        \"Commute Distance\", \"Teen Driver\",\n",
    "                        \"Personal Use\",\"Multi-Car\",\n",
    "                        \"Pleasure Use\",\"Family Vehicle\",\n",
    "                        \"Occasional Driver\",\"Good Student\",\n",
    "                        \"Student Discount\",\"Uninsured Motorist\",\n",
    "                        \"Home Address\"]\n",
    "\n",
    "commercial_auto_bigrams = [\"commercial auto\", \"commercial car\",\n",
    "                          \"Federal Tax\",\"Motor Carrier\",\n",
    "                          \"Cargo Coverage\",\"Company Name\",\n",
    "                          \"DOT Number\",\"Gross Vehicle\",\n",
    "                          \"For Hire\",\"Business Operations\",\n",
    "                          \"Employee Driver\",\"Fleet Size\",\n",
    "                          \"Hazardous Materials\",\"Terminal Address\"]\n",
    "\n",
    "\n",
    "\n",
    "personal_property_bigrams = [\"personal property\", \"private property\",\n",
    "                             \"Dwelling Location\", \"Replacement Cost\", \"Roof Age\", \"Fire Protection Class\", \"Swimming Pool\", \"Maisonry\"\n",
    "                             ]\n",
    "commercial_property_bigrams = [\"commercial property\"]\n",
    "\n",
    "\n",
    "# TRIGRAMS\n",
    "personal_auto_trigrams = [\"Number of Drivers\",\"High School Diploma\",\n",
    "\"Safe Driver Discount\",\"Annual Mileage Driven\",\n",
    "\"Driving Record History\",\"Coverage for Rental\",\n",
    "\"Primary Garaging Location\",\"Anti-Theft Device\",\n",
    "\"Resident of Household\"]\n",
    "\n",
    "\n",
    "commercial_auto_trigrams = [\"commercial fleet insurance\",\"Gross Vehicle Weight\",\n",
    "\"Interstate Commerce Commission\",\"Business Use Only\",\n",
    "\"Operating Radius Limit\",\"Combined Single Limit\",\n",
    "\"Certificate of Insurance\",\"Number of Employees\",\n",
    "\"Non-Owned Hired Auto\",\"Unified Carrier Registration\",\n",
    "\"Products Completed Operations\"]\n",
    "\n",
    "personal_property_trigrams = [\"personal property\", \"private property\"]\n",
    "commercial_property_trigrams = [\"commercial property\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfac9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Loads a PDF file from a local repository into a bytes object.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        The content of the PDF file as bytes.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"Error: The file was not found at {file_path}\")\n",
    "\n",
    "    # print(f\"Loading file: {path.name} ({os.path.getsize(path)} bytes)\")\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    return pdf_bytes\n",
    "\n",
    "\n",
    "def semantic_search(target_list:list, text_list:list):\n",
    "    temp = []\n",
    "    for target_label in target_list:\n",
    "        for txt in text_list:\n",
    "            threshold = cosine_similarity(target_label, txt)\n",
    "            if threshold > 0.95: \n",
    "                temp.append(txt)\n",
    "\n",
    "        if len(temp) > 0: return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea4adcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_classify(pdf_bytes: bytes):\n",
    "    \"\"\"\n",
    "    Processes the PDF bytes using pypdfium2 to extract text from the first page.\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes: The PDF content as bytes.\n",
    "    \"\"\"\n",
    "    # print(\"Processing PDF content with pypdfium2...\")\n",
    "\n",
    "    # 1. Load the PDF document directly from the bytes object\n",
    "    # pypdfium2's PdfDocument.open() handles byte streams naturally.\n",
    "    try:\n",
    "        pdf_document = pypdfium2.PdfDocument(pdf_bytes)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open PDF from bytes. Ensure the file is a valid PDF. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    num_pages = len(pdf_document)\n",
    "    # print(f\"Document successfully loaded. Total pages: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        print(\"Document is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2. Access the first page\n",
    "    page_index = 0\n",
    "    page = pdf_document.get_page(page_index)\n",
    "\n",
    "    form_type = set()\n",
    "    claim_type = set()\n",
    "    claim_cat = set()\n",
    "\n",
    "    # 3. Use the PDF Text Page object for Logo detection\n",
    "    for i in [70, 80, 90, 100, 150, 200]:\n",
    "        bitmap = page.render(scale=300/i)\n",
    "        image = bitmap.to_pil()\n",
    "\n",
    "        # Extract Text using Tesseract\n",
    "        text1 = pytesseract.image_to_string(image)\n",
    "\n",
    "        # Clean Text\n",
    "        text2 = clean_ocr_text(text1.lower())\n",
    "        clean_text = text2.split()\n",
    "\n",
    "        # Create bi/tri-grams\n",
    "        two_word_phrases = [' '.join(pair) for pair in ngrams(clean_text, 2)]\n",
    "        three_word_phrases = [' '.join(trio) for trio in ngrams(clean_text, 3)]\n",
    "\n",
    "        # Classify auto vs prop\n",
    "        if  (\"auto\" not in claim_cat) and \\\n",
    "            (semantic_search(auto_words, clean_text) or \\\n",
    "             semantic_search(auto_bigrams, two_word_phrases) or\\\n",
    "             semantic_search(auto_trigrams, three_word_phrases)):\n",
    "            claim_cat.add(\"auto\")\n",
    "\n",
    "        if  (\"prop\" not in claim_cat) and \\\n",
    "            (semantic_search(prop_words, clean_text) or \\\n",
    "             semantic_search(prop_bigrams, two_word_phrases) or\\\n",
    "             semantic_search(prop_trigrams, three_word_phrases)):\n",
    "            claim_cat.add(\"prop\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if (\"personal auto\" not in claim_type) and (semantic_search(personal_auto_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"personal auto\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(commercial_auto_bigrams, two_word_phrases) or semantic_search(commercial_auto_trigrams, three_word_phrases)):\n",
    "        #     claim_type.add(\"commercial auto\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(personal_property_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"personal property\")\n",
    "        # elif (\"personal auto\" not in claim_type) and (semantic_search(commercial_property_bigrams, two_word_phrases)):\n",
    "        #     claim_type.add(\"commercial property\")\n",
    "\n",
    "    return claim_cat\n",
    "\n",
    "\n",
    "        # text_page = page.get_textpage()\n",
    "        \n",
    "        # # Extract all text from the page\n",
    "        # text2 = text_page.get_text_range()\n",
    "\n",
    "        # # 4. Print the extracted text\n",
    "        # print(\"-\" * 50)\n",
    "        # print(f\"Extracted Text from Page {page_index + 1}:\")\n",
    "        # print(text2.strip()[:500] + ('...' if len(text2) > 500 else '')) # Print first 500 chars\n",
    "        # print(\"-\" * 50)\n",
    "    # print(f\"Claim Type is: {claim_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af2a1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recursive search in: /home/aamir79/projects/quote-management-system/sample_forms\n",
      "Found 32 PDF file(s).\n",
      "\n",
      "--- File 1/32: ../sample_forms/auto/Aviva-private-car-insurance-proposal-form.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 2/32: ../sample_forms/auto/CSIO-Commercial-Fleet-application-form.pdf ---\n",
      "set()\n",
      "\n",
      "--- File 3/32: ../sample_forms/auto/Maritime-Motor-Insurance-Quotation-Form.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 4/32: ../sample_forms/auto_commercial/ACORD-Business-Auto-Section-127.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 5/32: ../sample_forms/auto_commercial/Acord125 Commercial App.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 6/32: ../sample_forms/auto_commercial/Commonwealth-commercial-quote_info_sheet.pdf ---\n",
      "set()\n",
      "\n",
      "--- File 7/32: ../sample_forms/auto_commercial/Truckers-Quick-Quote-Sheet.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 8/32: ../sample_forms/auto_personal/Acord-71.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 9/32: ../sample_forms/auto_personal/Acord-83-Personal-Umbrella.pdf ---\n",
      "{'auto', 'prop'}\n",
      "\n",
      "--- File 10/32: ../sample_forms/auto_personal/Acord-90-CA.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 11/32: ../sample_forms/auto_personal/Acord-90-KS.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 12/32: ../sample_forms/auto_personal/Acord-90-MI.pdf ---\n",
      "{'auto', 'prop'}\n",
      "\n",
      "--- File 13/32: ../sample_forms/auto_personal/acord-90-NY-fillable.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 14/32: ../sample_forms/auto_personal/Acord-90-NY.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 15/32: ../sample_forms/auto_personal/Auto Insurance Quote Form.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 16/32: ../sample_forms/auto_personal/Auto Insurance Quote Request.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 17/32: ../sample_forms/auto_personal/Auto-Quote-Example.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 18/32: ../sample_forms/auto_personal/Auto-quote-TX-Harmon.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 19/32: ../sample_forms/auto_personal/Kleve-Auto-Quote-OH.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 20/32: ../sample_forms/auto_personal/National-General-Auto-Quote.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 21/32: ../sample_forms/auto_personal/State-Farm-Auto-Quotes.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 22/32: ../sample_forms/property_commercial/Acord-140-Property.pdf ---\n",
      "{'auto', 'prop'}\n",
      "\n",
      "--- File 23/32: ../sample_forms/property_commercial/Acord-140.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 24/32: ../sample_forms/property_commercial/Acord125CommInsApp.pdf ---\n",
      "{'auto'}\n",
      "\n",
      "--- File 25/32: ../sample_forms/property_commercial/Business Insurance Quote Request Form.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 26/32: ../sample_forms/property_personal/Acord-80-Personal-Property-filled.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 27/32: ../sample_forms/property_personal/Acord-80-Personal-Property.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 28/32: ../sample_forms/property_personal/Buffalo-homeowners-quote.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 29/32: ../sample_forms/property_personal/Home-Quote-Comparison-Example.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 30/32: ../sample_forms/property_personal/Homeowners Insurance Quote Form.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 31/32: ../sample_forms/property_personal/Renters Insurance Quote.pdf ---\n",
      "{'prop'}\n",
      "\n",
      "--- File 32/32: ../sample_forms/property_personal/Student Accident Insurance Quote Request Form.pdf ---\n",
      "{'prop'}\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "root_path = Path(\"../sample_forms/\")\n",
    "\n",
    "if not root_path.is_dir():\n",
    "    print(f\"Error: Root directory not found\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Starting recursive search in: {root_path.resolve()}\")\n",
    "\n",
    "# Use glob with '**/*.pdf' for recursive search for all files ending in .pdf\n",
    "pdf_files = list(root_path.glob('**/*.pdf'))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"No PDF files found.\")\n",
    "    exit(0)\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF file(s).\")\n",
    "pdf_paths = list(map(str, pdf_files))\n",
    "file_classify = pd.DataFrame(pdf_paths, columns=[\"File_Path\"])\n",
    "file_classify[\"Claim_Type\"] = None\n",
    "file_classify[\"Form_Type\"] = None\n",
    "\n",
    "for i in range(len(pdf_paths)):\n",
    "    print(f\"\\n--- File {i+1}/{len(pdf_paths)}: {pdf_paths[i]} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load the entire file content into a bytes object using pathlib's method\n",
    "        pdf_bytes = load_pdf(pdf_paths[i])\n",
    "        \n",
    "        # Process the loaded bytes\n",
    "        # print(process_pdf(pdf_bytes))\n",
    "        val = quote_classify(pdf_bytes)\n",
    "        print(val)\n",
    "        # file_classify.at[i,\"Claim_Category\"] = val\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"    [SKIP] Permission denied when accessing {pdf_paths[i]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [ERROR] An unexpected error occurred while reading {pdf_paths[i]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b838317",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f349d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pdf_paths[14]\n",
    "print(a)\n",
    "b = load_pdf(a)\n",
    "c = process_pdf(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3624a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: You must set the path to the Tesseract executable \n",
    "# if it is not automatically found by pytesseract.\n",
    "# \n",
    "# Example for Windows:\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def ocr_scanned_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs OCR on a scanned PDF file and returns the extracted text.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: The file path to the scanned PDF.\n",
    "\n",
    "    Returns:\n",
    "        A string containing all text extracted from the PDF, \n",
    "        or an error message if the file cannot be processed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return f\"Error: PDF file not found at {pdf_path}\"\n",
    "\n",
    "    try:\n",
    "        # Load the PDF file using pypdfium2\n",
    "        pdf_document = pdfium.PdfDocument(pdf_path)\n",
    "        num_pages = len(pdf_document)\n",
    "        full_text = []\n",
    "\n",
    "        print(f\"Starting OCR on {num_pages} pages...\")\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = pdf_document.get_page(i)\n",
    "            \n",
    "            # Render the page to a bitmap (image)\n",
    "            # Scale factor 2 provides good resolution for OCR\n",
    "            bitmap = page.render(scale=2)\n",
    "            \n",
    "            # Convert the bitmap to a PIL Image object\n",
    "            image = bitmap.to_pil()\n",
    "            \n",
    "            # --- OCR Processing ---\n",
    "            # Use pytesseract to extract text from the image\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            \n",
    "            print(f\"--- Page {i+1} OCR Completed ---\")\n",
    "            full_text.append(text)\n",
    "\n",
    "        return \"\\n\".join(full_text)\n",
    "\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        return \"Error: Tesseract is not installed or not in your PATH. Please install it or set 'pytesseract.pytesseract.tesseract_cmd'.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during processing: {e}\"\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'path/to/your/scanned_document.pdf' with your actual file path\n",
    "    pdf_file = 'path/to/your/scanned_document.pdf' \n",
    "    \n",
    "    # --- IMPORTANT: Ensure you have a scanned PDF file here for testing ---\n",
    "    # For demonstration, we'll use a placeholder path:\n",
    "    # If you want to test this, create a simple PDF with a picture of text.\n",
    "    \n",
    "    # Example using a dummy file path (update this):\n",
    "    dummy_pdf_file = \"sample_scanned_document.pdf\" \n",
    "    \n",
    "    # Assuming 'sample_scanned_document.pdf' exists and is a scanned image PDF\n",
    "    if os.path.exists(dummy_pdf_file):\n",
    "        extracted_text = ocr_scanned_pdf(dummy_pdf_file)\n",
    "        \n",
    "        print(\"\\n====================================\")\n",
    "        print(\"         EXTRACTED TEXT\")\n",
    "        print(\"====================================\\n\")\n",
    "        print(extracted_text)\n",
    "    else:\n",
    "        print(f\"Please replace '{dummy_pdf_file}' with the path to an existing scanned PDF to run the example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
